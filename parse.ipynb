{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import psycopg2           # PostGIS\n",
    "import re                 # Regex\n",
    "from lxml import etree    # XML parsing\n",
    "from IPython.display import clear_output\n",
    "\n",
    "infile = \"testdump-m.xml\"\n",
    "lang = \"en\"\n",
    "\n",
    "ns = '{http://www.mediawiki.org/xml/export-0.10/}'\n",
    "\n",
    "# drop everything and start over?\n",
    "drop = True\n",
    "\n",
    "# Connect to an existing database\n",
    "conn = psycopg2.connect(\"dbname=CPT user=carsten\")\n",
    "\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()\n",
    "\n",
    "# let's pre-compile some regexes\n",
    "linkpattern = re.compile(\"\\[\\[?([^]|]*)(\\|)?([^]|]*)?\\]\\]\")\n",
    "\n",
    "if drop: # drop everything and recreate tables\n",
    "    cur.execute('DROP TABLE IF EXISTS \"links\";')\n",
    "    cur.execute('CREATE TABLE \"links\" (\"from\" varchar, \"to\" varchar, \"lang\" varchar, \"links\" integer, \"mentions\" integer) ;')\n",
    "    \n",
    "def findreferences(pagetext, pagetitle):\n",
    "    \n",
    "    # find all links via regex, save in a dict with the link as key and number of occurrences for this link as value\n",
    "    links = linkpattern.findall(pagetext)\n",
    "    \n",
    "    # we'll go through the links in alphabetical order; whenever the lastlink is different from the current one, we'll \n",
    "    # write the accumulated count of the lastlinks to the DB:\n",
    "    lastlink = None \n",
    "    lastalias = None\n",
    "    linkscount = 0\n",
    "    mentionscount = 0\n",
    "    \n",
    "    for match in sorted(links):\n",
    "        link = match[0]\n",
    "\n",
    "        theselinks = 0\n",
    "        \n",
    "        if link != lastlink: \n",
    "            #write to DB:\n",
    "            if lastlink:  # don't write on the first iteration when lastlink is empty!\n",
    "                # insert results into DB\n",
    "                cur.execute(\"INSERT INTO links VALUES (%s, %s, %s, %s, %s);\", (pagetitle, lastlink, lang, linkscount, mentionscount))\n",
    "                \n",
    "            # and start over\n",
    "            lastlink = link\n",
    "            linkscount = 1\n",
    "        \n",
    "            # find all occurrences of the link text on the page:\n",
    "            matches = re.findall(re.escape(link), pagetext)\n",
    "            theselinks = len(matches)\n",
    "            mentionscount = theselinks\n",
    "            \n",
    "        else:\n",
    "            # still the same link, only update the linkscount\n",
    "            linkscount = linkscount + 1\n",
    "\n",
    "        # if there is an alias in this link, also look for its occurrences: \n",
    "        if match[2]:  # this is the alias\n",
    "            alias = match[2].strip(\" \")\n",
    "            if len(alias) > 0: # skips empty alias, which does happen...\n",
    "                # only search for appearances of this alias if it is not the same as in the last iteration!\n",
    "                if alias != lastalias: \n",
    "                    lastalias = alias\n",
    "                    aliasmatches = re.findall(re.escape(alias), pagetext)\n",
    "\n",
    "                    # if the alias is a substring of the full page title, e.g. \"Brooklyn, NY\" and \"Brooklyn\"\n",
    "                    # avoid double counting!\n",
    "                    if alias in link:\n",
    "                        mentionscount = mentionscount + len(aliasmatches) - theselinks\n",
    "                    else:\n",
    "                        mentionscount = mentionscount + len(aliasmatches)   \n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "# for the parsing, we follow the approach explained here: \n",
    "# http://www.ibm.com/developerworks/xml/library/x-hiperfparse/ \n",
    "pages = etree.iterparse(infile, events=('end',), tag=ns+'page')\n",
    "\n",
    "# go through wikipedia pages in dump, one by one:\n",
    "for event, page in pages: \n",
    "    pagetitle    = page.find(ns+'title')\n",
    "    pagetext = page.find(ns+'revision/'+ns+'text')\n",
    "\n",
    "    #print \" >> \"+pagetitle.text\n",
    "    findreferences(pagetext.text, pagetitle.text)\n",
    "\n",
    "    # print(title.text)\n",
    "    # It's safe to call clear() here because no descendants will be accessed\n",
    "    page.clear()\n",
    "\n",
    "    # Also eliminate now-empty references from the root node to <Title> \n",
    "    while page.getprevious() is not None:\n",
    "        del page.getparent()[0]\n",
    "\n",
    "\n",
    "# Pass data to fill\n",
    "# cur.execute(\"INSERT INTO test (num, data) VALUES (%s, %s)\",\n",
    "\n",
    "# Query the database and obtain data as Python objects\n",
    "#cur.execute(\"SELECT * FROM test;\")\n",
    "#cur.fetchone()\n",
    "\n",
    "\n",
    "# Make the changes to the database persistent\n",
    "conn.commit()\n",
    "\n",
    "# Close communication with the database\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
