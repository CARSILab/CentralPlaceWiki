{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import re\n",
    "from lxml import etree\n",
    "\n",
    "infile = \"testdump-s.xml\"\n",
    "lang = \"en\"\n",
    "\n",
    "ns = '{http://www.mediawiki.org/xml/export-0.10/}'\n",
    "\n",
    "# drop everything and start over?\n",
    "drop = False\n",
    "\n",
    "# Connect to an existing database\n",
    "conn = psycopg2.connect(\"dbname=CPT user=carsten\")\n",
    "\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()\n",
    "\n",
    "# let's pre-compile some regexes\n",
    "linkpattern = re.compile(\"\\[\\[?([^]|]*)(\\|)?([^]|]*)?\\]\\]\")\n",
    "\n",
    "if drop: # drop everything and recreate tables\n",
    "    cur.execute('DROP TABLE IF EXISTS \"links\";')\n",
    "    cur.execute('CREATE TABLE \"links\" (\"from\" varchar, \"to\" varchar, \"lang\" varchar, \"links\" integer, \"mentions\" integer) ;')\n",
    "\n",
    "\n",
    "    \n",
    "def findreferences(pagetext):\n",
    "    # find all links via regex, save in a dict with the link as key and number of occurrences for this link as value\n",
    "    links = linkpattern.finditer(pagetext)\n",
    "    for match in links:\n",
    "        print match.group(0)\n",
    "        \n",
    "        title = match.group(1)\n",
    "        print title\n",
    "        \n",
    "        if match.group(2):\n",
    "            alias = match.group(3)\n",
    "            print alias\n",
    "    # 2. go through array of links; for every link: \n",
    "    #      a. find all mentions via regex (also for aliases!)\n",
    "    #      b. insert into PostGIS links table\n",
    "    \n",
    "    \n",
    "    \n",
    "# for the parsing, we follow the approach explained here: \n",
    "# http://www.ibm.com/developerworks/xml/library/x-hiperfparse/ \n",
    "pages = etree.iterparse(infile, events=('end',), tag=ns+'page')\n",
    "\n",
    "# go through wikipedia pages in dump, one by one:\n",
    "for event, page in pages:    \n",
    "    title    = page.find(ns+'title')\n",
    "    pageid   = page.find(ns+'id')    \n",
    "    pagetext = page.find(ns+'revision/'+ns+'text')\n",
    "    \n",
    "    print \" >> \"+title.text\n",
    "    findreferences(pagetext.text)\n",
    "    \n",
    "    # print(title.text)\n",
    "    # It's safe to call clear() here because no descendants will be accessed\n",
    "    page.clear()\n",
    "\n",
    "    # Also eliminate now-empty references from the root node to <Title> \n",
    "    while page.getprevious() is not None:\n",
    "        del page.getparent()[0]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# Pass data to fill\n",
    "# cur.execute(\"INSERT INTO test (num, data) VALUES (%s, %s)\",\n",
    "\n",
    "# Query the database and obtain data as Python objects\n",
    "#cur.execute(\"SELECT * FROM test;\")\n",
    "#cur.fetchone()\n",
    "\n",
    "\n",
    "# Make the changes to the database persistent\n",
    "conn.commit()\n",
    "\n",
    "# Close communication with the database\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
