{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.2490639687\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import psycopg2               # PostGIS client\n",
    "import re                     # Regex\n",
    "from lxml import etree        # XML parsing\n",
    "import timeit\n",
    "\n",
    "infile = \"rego-dump.xml\"\n",
    "lang = \"en\"  # so that we can keep track of different languages in our DB\n",
    "\n",
    "ns = '{http://www.mediawiki.org/xml/export-0.10/}'\n",
    "\n",
    "# drop everything and start over?\n",
    "drop = False\n",
    "\n",
    "# Connect to an existing database\n",
    "conn = psycopg2.connect(\"dbname=CPT user=carsten\")\n",
    "\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()\n",
    "\n",
    "# let's pre-compile some regexes\n",
    "linkpattern = re.compile(\"\\[\\[?([^]|]*)(\\|)?([^]|]*)?\\]\\]\")\n",
    "\n",
    "if drop: # drop everything and recreate tables\n",
    "    cur.execute('DROP TABLE IF EXISTS \"links\";')\n",
    "    cur.execute('CREATE TABLE \"links\" (\"from\" varchar, \"to\" varchar, \"lang\" varchar, \"links\" integer, \"mentions\" integer) ;')\n",
    "    \n",
    "def findreferences(page):\n",
    "        \n",
    "    pagetitle = page.find(ns+'title').text\n",
    "    pagetext  = page.find(ns+'revision/'+ns+'text').text\n",
    "    \n",
    "    # find all links via regex, save in a dict with the link as key and number of occurrences for this link as value\n",
    "    links = linkpattern.findall(pagetext)\n",
    "    \n",
    "    # we'll go through the links in alphabetical order; whenever the lastlink is different from the current one, we'll \n",
    "    # write the accumulated count of the lastlinks to the DB:\n",
    "    lastlink = None \n",
    "    lastalias = None\n",
    "    linkscount = 0\n",
    "    mentionscount = 0\n",
    "    \n",
    "    for match in sorted(links):\n",
    "        link = match[0]\n",
    "\n",
    "        theselinks = 0\n",
    "        \n",
    "        if link != lastlink: \n",
    "            #write to DB:\n",
    "            if lastlink:  # don't write on the first iteration when lastlink is empty!\n",
    "                # insert results into DB\n",
    "                cur.execute(\"INSERT INTO links VALUES (%s, %s, %s, %s, %s);\", (pagetitle, lastlink, lang, linkscount, mentionscount))\n",
    "                \n",
    "            # and start over\n",
    "            lastlink = link\n",
    "            linkscount = 1\n",
    "        \n",
    "            # find all occurrences of the link text on the page:\n",
    "            matches = re.findall(re.escape(link), pagetext)\n",
    "            theselinks = len(matches)\n",
    "            mentionscount = theselinks\n",
    "            \n",
    "        else:\n",
    "            # still the same link, only update the linkscount\n",
    "            linkscount = linkscount + 1\n",
    "\n",
    "        # if there is an alias in this link, also look for its occurrences: \n",
    "        if match[2]:  # this is the alias\n",
    "            alias = match[2].strip(\" \")\n",
    "            if len(alias) > 0: # skips empty alias, which does happen...\n",
    "                # only search for appearances of this alias if it is not the same as in the last iteration!\n",
    "                if alias != lastalias: \n",
    "                    lastalias = alias\n",
    "                    aliasmatches = re.findall(re.escape(alias), pagetext)\n",
    "\n",
    "                    # if the alias is a substring of the full page title, e.g. \"Brooklyn, NY\" and \"Brooklyn\"\n",
    "                    # avoid double counting!\n",
    "                    if alias in link:\n",
    "                        mentionscount = mentionscount + len(aliasmatches) - theselinks\n",
    "                    else:\n",
    "                        mentionscount = mentionscount + len(aliasmatches)   \n",
    "                        \n",
    "    page.clear()\n",
    "\n",
    "    # Also eliminate now-empty references from the root node to the page \n",
    "    while page.getprevious() is not None:\n",
    "        del page.getparent()[0]\n",
    "            \n",
    "            \n",
    "# for the parsing, we follow the approach explained here: \n",
    "# http://www.ibm.com/developerworks/xml/library/x-hiperfparse/ \n",
    "pages = etree.iterparse(infile, events=('end',), tag=ns+'page')\n",
    "\n",
    "def go():\n",
    "    # go through wikipedia pages in dump, one by one:\n",
    "    for event, page in pages: \n",
    "        findreferences(page)\n",
    "\n",
    "print timeit.timeit(go, 'gc.enable()')\n",
    "\n",
    "# Make the changes to the database persistent\n",
    "conn.commit()\n",
    "\n",
    "# Close communication with the database\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.916877985\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import re                     # Regex\n",
    "from lxml import etree        # XML parsing\n",
    "import timeit\n",
    "\n",
    "infile = \"rego-dump.xml\"\n",
    "lang = \"en\"  # so that we can keep track of different languages in our DB\n",
    "\n",
    "ns = '{http://www.mediawiki.org/xml/export-0.10/}'\n",
    "\n",
    "\n",
    "# these control how many copies of this script will be started, and which of those \n",
    "# this particular instance takes care of. Can be used to only parse every n-th page.\n",
    "# TODO: Move these to command line args later!\n",
    "numthreads = 200\n",
    "thisthread = 1\n",
    "\n",
    "currentpage = 1\n",
    "\n",
    "sqlfile = open(\"insert-\"+str(thisthread)+\"-of-\"+str(numthreads)+\".sql\", \"a\")\n",
    "\n",
    "# let's pre-compile some regexes\n",
    "linkpattern = re.compile(\"\\[\\[?([^]|]*)(\\|)?([^]|]*)?\\]\\]\")\n",
    "    \n",
    "def findreferences(page):\n",
    "    \n",
    "    global sqlfile\n",
    "        \n",
    "    pagetitle = page.find(ns+'title').text\n",
    "    pagetext  = page.find(ns+'revision/'+ns+'text').text\n",
    "    \n",
    "    # find all links via regex, save in a dict with the link as key and number of occurrences for this link as value\n",
    "    links = linkpattern.findall(pagetext)\n",
    "    \n",
    "    # we'll go through the links in alphabetical order; whenever the lastlink is different from the current one, we'll \n",
    "    # write the accumulated count of the lastlinks to the DB:\n",
    "    lastlink = None \n",
    "    lastalias = None\n",
    "    linkscount = 0\n",
    "    mentionscount = 0\n",
    "    \n",
    "    for match in sorted(links):\n",
    "        link = match[0]\n",
    "\n",
    "        theselinks = 0\n",
    "        \n",
    "        if link != lastlink: \n",
    "            #write to DB:\n",
    "            if lastlink:  # don't write on the first iteration when lastlink is empty!\n",
    "                # create statement to insert results into DB\n",
    "                insert = u\"INSERT INTO links VALUES ('\"+pagetitle.replace(\"'\", \"''\")+\"', '\"+lastlink.replace(\"'\", \"''\")+\"', '\"+lang+\"', \"+str(linkscount)+\", \"+str(mentionscount)+\");\\n\"\n",
    "                sqlfile.write(insert.encode('utf8'))\n",
    "                \n",
    "            # and start over\n",
    "            lastlink = link\n",
    "            linkscount = 1\n",
    "        \n",
    "            # find all occurrences of the link text on the page:\n",
    "            matches = re.findall(re.escape(link), pagetext)\n",
    "            theselinks = len(matches)\n",
    "            mentionscount = theselinks\n",
    "            \n",
    "        else:\n",
    "            # still the same link, only update the linkscount\n",
    "            linkscount = linkscount + 1\n",
    "\n",
    "        # if there is an alias in this link, also look for its occurrences: \n",
    "        if match[2]:  # this is the alias\n",
    "            alias = match[2].strip(\" \")\n",
    "            if len(alias) > 0: # skips empty alias, which does happen...\n",
    "                # only search for appearances of this alias if it is not the same as in the last iteration!\n",
    "                if alias != lastalias: \n",
    "                    lastalias = alias\n",
    "                    aliasmatches = re.findall(re.escape(alias), pagetext)\n",
    "\n",
    "                    # if the alias is a substring of the full page title, e.g. \"Brooklyn, NY\" and \"Brooklyn\"\n",
    "                    # avoid double counting!\n",
    "                    if alias in link:\n",
    "                        mentionscount = mentionscount + len(aliasmatches) - theselinks\n",
    "                    else:\n",
    "                        mentionscount = mentionscount + len(aliasmatches)   \n",
    "                        \n",
    "    page.clear()\n",
    "\n",
    "    # Also eliminate now-empty references from the root node to the page \n",
    "    while page.getprevious() is not None:\n",
    "        del page.getparent()[0]\n",
    "            \n",
    "            \n",
    "# for the parsing, we follow the approach explained here: \n",
    "# http://www.ibm.com/developerworks/xml/library/x-hiperfparse/ \n",
    "pages = etree.iterparse(infile, events=('end',), tag=ns+'page')\n",
    "\n",
    "def go():\n",
    "    \n",
    "    global numthreads\n",
    "    global thisthread\n",
    "    global currentpage\n",
    "    global sqlfile\n",
    "    \n",
    "    # go through wikipedia pages in dump, one by one:\n",
    "    for event, page in pages:   \n",
    "       \n",
    "        # make sure we only parse every n-th page!\n",
    "        if currentpage == thisthread:\n",
    "            findreferences(page)\n",
    "\n",
    "        currentpage = currentpage + 1\n",
    "        if currentpage > numthreads:\n",
    "            currentpage = 1\n",
    "\n",
    "    sqlfile.close()\n",
    "            \n",
    "print timeit.timeit(go, 'gc.enable()')\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
